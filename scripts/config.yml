hparams: &HPARAMS
  n_head: 12
  n_ctx: 1024
  n_layer: 12
  n_embd: 768
  embed_dropout: 0.1
  n_vocab: 102400  # BPE 的 Vocab 长度。注意这个一定要和 BPE 模型一致！！！

input: &INPUT
  data_files: ~
  tokens_sample_rate_min: 0.5
  tokens_sample_rate_max: 0.9
  repeat_count: 1
  shuffle_buffer_size: 1
  batch_size: 1
  prefetch_buffer_size: 0

epochs: 30

estimator:
  hparams:
    <<: *HPARAMS

  train_steps: 100
  train_input:
    <<: *INPUT
    data_files: ../../data/wiki_zh.train.tfrecords

  eval_steps: 10
  eval_input:
    <<: *INPUT
    data_files: ../../data/wiki_zh.valid.tfrecords

  run:  # ref: https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig
    model_dir: ../out
    save_summary_steps: 1

  optimizer:
    lr: .25e-5
    warmup_steps: 2000
    beta1: .9e-1
    beta2: .98e-2
    epsilon: 1.e-9
    opt_name: adam
    weight_decay: 1.e-2
    train_batch_size: 1
    attn_dropout: 0.1
    res_dropout: 0.1
    predict_batch_size: 1
    eval_batch_size: 1
    scale_by_depth": true
    scale_by_in": true

    iterations: 300
    train_steps: 10000
    eval_steps: 10
    max_steps: 800000
