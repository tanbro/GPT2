hparams: &HPARAMS
  n_head: 12
  n_ctx: 1024
  n_layer: 12
  n_embd: 768
  embed_dropout: 0.1
  n_vocab: 102400  # BPE 的 Vocab 长度。注意这个一定要和 BPE 模型一致！！！

input: &INPUT
  interleave_cycle_length: 4
  shuffle_buffer_size: 128
  batch_size: 1
  prefetch_buffer_size: 256
  tokens_sample_rate_min: 0.5
  tokens_sample_rate_max: 0.9

estimator:
  hparams:
    <<: *HPARAMS

  train_input:
    <<: *INPUT
    data_files: ../../data/wiki_zh.train.tfrecords

  eval_input:
    <<: *INPUT
    data_files: ../../data/wiki_zh.train.tfrecords

  # 输出目录
  model_dir: ../out

  # train loop params
  lr: .25e-5
  warmup_steps: 2000
  beta1: .9e-1
  beta2: .98e-2
  epsilon: 1.e-9
  opt_name: adam
  weight_decay: 1.e-2
  train_batch_size: 1
  attn_dropout: 0.1
  res_dropout: 0.1
  predict_batch_size: 1
  eval_batch_size: 1
  scale_by_depth": true
  scale_by_in": true

  iterations: 300
  train_steps: 10000
  eval_steps: 10
  max_steps: 800000

eval:
  hparams:
    <<: *HPARAMS

  input:
    <<: *INPUT
    data_files: ../../data/wiki_zh.valid.tfrecords
