These are a sample of outputs from my 1.5B model, using the same inputs as the [original GPT2 post](https://openai.com/blog/better-language-models/). The particular checkpoint used to generate these samples was trained for 3-4 days on a v3-512 pod, which should make it about equivalent to OpenAI's model. The final 1.5B will be trained for a bit longer. The examplesX.txt files were generated with top_k set to 40, the files examplesX_free.txt had top_k disabled. The non-free examples tend to have higher quality, but less variety and are prone to repeating themselves.

Unlike in OpenAI's blogpost, I did absolutely nothing to manually cherry pick the quality here. Some examples are good, most are bad. Almost all outputs decay in quality as the post gets longer. I wanted to keep the raw outputs to give an accurate feel of the strengths and failure cases of the model. The truth is that using a GPT2 type model to create text you want is more an art than a science and can be very finicky. 

The token "<|endoftext|>" is an artifact of the training process. Basically during training, multiple texts were "glued" together with an "<|endoftext|>" in between them. So when you see an <|endoftext|>, the AI thinks that text is done and is "moving on to something else".
